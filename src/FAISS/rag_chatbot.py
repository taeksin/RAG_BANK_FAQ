import os
import streamlit as st
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_KEY"] = api_key

# Streamlit ê¸°ë³¸ ì„¤ì •
st.set_page_config(page_title="RAG ê¸°ë°˜ ì±—ë´‡", layout="wide")
st.title("RAG ê¸°ë°˜ FAQ ì±—ë´‡ ğŸ¤–")

# ì„ë² ë”© ëª¨ë¸ ìƒì„±
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# í˜„ì¬ íŒŒì¼ ê¸°ì¤€ìœ¼ë¡œ ë‘ ë‹¨ê³„ ìƒìœ„ í´ë”ë¡œ ì´ë™ í›„ FAISS ì €ì¥ì†Œ ê²½ë¡œ ì„¤ì •
base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
faiss_path = os.path.join(base_dir, "data/faiss_index")

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
vectorstore = FAISS.load_local(faiss_path, embeddings=embedding_model, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs={'k': 5, 'fetch_k': 10, 'lambda_mult': 0.9})

# RAG êµ¬ì„± ìš”ì†Œ ì„¤ì •
prompt = hub.pull("fas_rag_platformdata")
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.5)
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# ì§ˆë¬¸ ì…ë ¥ ë° ì—”í„°í‚¤ë¡œ ì œì¶œ
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì¸í„°ë„·/ìŠ¤ë§ˆíŠ¸ë±…í‚¹ ì´ì²´í•œë„ ì¡°íšŒ ë° ì¦ì•¡í•˜ëŠ” ë°©ë²• ì•Œë ¤ì¤˜", on_change=lambda: st.session_state.submit_question(), key="question_input")

# ì„¸ì…˜ ìƒíƒœë¡œ ë²„íŠ¼ ë™ì‘ ê´€ë¦¬
if "submit_question" not in st.session_state:
    st.session_state.submit_question = lambda: None

# "ì§ˆë¬¸í•˜ê¸°" ë²„íŠ¼ ë™ì‘
if st.button("ì§ˆë¬¸í•˜ê¸°") or st.session_state.submit_question:
    st.session_state.submit_question = lambda: None  # ì´ˆê¸°í™”
    if question.strip():
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            # ì§ˆë¬¸ì„ ì„ë² ë”©
            question_embedding = embedding_model.embed_query(question)

            # ë¦¬íŠ¸ë¦¬ë²„ì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
            retrieved_documents = vectorstore.similarity_search_by_vector(question_embedding, k=5)

            # RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
            response = rag_chain.invoke(question)

            # ì‘ë‹µ ì¶œë ¥
            st.subheader("ì±—ë´‡ ì‘ë‹µ:")
            st.write(response)

            # ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶œë ¥ (ì „ì²´ë¥¼ í•˜ë‚˜ì˜ í† ê¸€ë¡œ í‘œì‹œ)
            st.subheader("ê²€ìƒ‰ëœ ë¬¸ì„œ:")
            with st.expander("ê²€ìƒ‰ëœ ë¬¸ì„œ ë³´ê¸° (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)"):

                for idx, doc in enumerate(retrieved_documents[:5], 1):  # ìµœëŒ€ 5ê°œ ë¬¸ì„œë§Œ ì¶œë ¥
                    st.write(f"### ë¬¸ì„œ {idx}:")
                    st.write(f"- **ì€í–‰**: {doc.metadata.get('ì€í–‰', 'ì •ë³´ ì—†ìŒ')}")
                    st.write(f"- **1ì°¨ ë¶„ë¥˜**: {doc.metadata.get('1ì°¨ë¶„ë¥˜', 'ì •ë³´ ì—†ìŒ')}")
                    st.write(f"- **2ì°¨ ë¶„ë¥˜**: {doc.metadata.get('2ì°¨ë¶„ë¥˜', 'ì •ë³´ ì—†ìŒ')}")
                    st.write(f"- **ì§ˆë¬¸**: {doc.metadata.get('ì§ˆë¬¸', 'ì •ë³´ ì—†ìŒ')}")
                    st.write(f"- **ë‹µë³€**: {doc.metadata.get('ë‹µë³€', 'ì •ë³´ ì—†ìŒ')}")

                    # ë¬¸ì„œ ê°„ êµ¬ë¶„ì„ 
                    if idx < len(retrieved_documents[:5]):  # ë§ˆì§€ë§‰ ë¬¸ì„œ ì´í›„ì—ëŠ” êµ¬ë¶„ì„  ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                        st.markdown("---")  # êµ¬ë¶„ì„ 
    else:
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
