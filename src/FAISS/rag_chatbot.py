import os
import json
import streamlit as st
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="RAG ê¸°ë°˜ ì±—ë´‡", layout="wide")

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_KEY"] = api_key

# Streamlit ê¸°ë³¸ ì„¤ì •
st.title("RAG ê¸°ë°˜ FAQ ì±—ë´‡ ğŸ¤–")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "current_question" not in st.session_state:
    st.session_state.current_question = None
if "current_response" not in st.session_state:
    st.session_state.current_response = None
if "loading" not in st.session_state:
    st.session_state.loading = False

# JSON íŒŒì¼ ê²½ë¡œ ì„¤ì •
history_file_path = "chat_history.json"

# íˆìŠ¤í† ë¦¬ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
def load_chat_history():
    if os.path.exists(history_file_path):
        with open(history_file_path, "r", encoding="utf-8") as file:
            return json.load(file)
    return []

# íˆìŠ¤í† ë¦¬ ì €ì¥ í•¨ìˆ˜
def save_chat_history():
    try:
        with open(history_file_path, "w", encoding="utf-8") as file:
            json.dump(st.session_state.chat_history, file, ensure_ascii=False, indent=4)
    except Exception as e:
        st.error(f"íˆìŠ¤í† ë¦¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# # íˆìŠ¤í† ë¦¬ ë””ë²„ê¹…: JSON íŒŒì¼ì—ì„œ ë¡œë“œëœ ë°ì´í„° í™•ì¸
# st.write("ë””ë²„ê¹…: ë¡œë“œëœ íˆìŠ¤í† ë¦¬:", st.session_state.chat_history)

# ì„ë² ë”© ëª¨ë¸ ìƒì„±
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# FAISS ì €ì¥ì†Œ ê²½ë¡œ ì„¤ì •
base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
faiss_path = os.path.join(base_dir, "data/faiss_index")

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ
vectorstore = FAISS.load_local(faiss_path, embeddings=embedding_model, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs={'k': 5, 'fetch_k': 10, 'lambda_mult': 0.9})

# RAG êµ¬ì„± ìš”ì†Œ ì„¤ì •
prompt = hub.pull("fas_rag_platformdata")
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.5)
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# íˆìŠ¤í† ë¦¬ í‘œì‹œ í•¨ìˆ˜
def display_history(container):
    with container:
        if st.session_state.chat_history:
            st.subheader("íˆìŠ¤í† ë¦¬:")
            for i, entry in enumerate(st.session_state.chat_history):
                with st.expander(f"ì§ˆë¬¸ {i+1}: {entry['ì§ˆë¬¸']}"):
                    st.write(f"**ì§ˆë¬¸:** {entry['ì§ˆë¬¸']}")
                    st.write(f"**ë‹µë³€:** {entry['ì‘ë‹µ']}")

# UI ì»¨í…Œì´ë„ˆ ìƒì„±
history_container = st.container()

# ì´ˆê¸° íˆìŠ¤í† ë¦¬ í‘œì‹œ
display_history(history_container)

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
if user_input:
    # ì…ë ¥ê°’ ìœ íš¨ì„± ê²€ì‚¬
    if not st.session_state.loading:
        st.session_state.current_question = user_input
        st.session_state.loading = True  # ë¡œë”© ìƒíƒœ ì‹œì‘
        st.session_state.current_response = None

        # í˜„ì¬ ì§ˆë¬¸ì„ íˆìŠ¤í† ë¦¬ì— ì¦‰ì‹œ ì¶”ê°€ (ì‘ë‹µì€ ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸)
        st.session_state.chat_history.append({"ì§ˆë¬¸": user_input, "ì‘ë‹µ": "ì‘ë‹µ ìƒì„± ì¤‘..."})
    else:
        st.warning("í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µì´ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")

# ê¸°ì¡´ ì½”ë“œì—ì„œ ì‘ë‹µ ì²˜ë¦¬ ë¶€ë¶„
if st.session_state.loading and st.session_state.current_question:
    try:
        with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
            # RAG ì‘ë‹µ ìƒì„±
            retrieved_documents = retriever.invoke(st.session_state.current_question)
            response = rag_chain.invoke(st.session_state.current_question)

            # ì‘ë‹µ ì €ì¥
            st.session_state.current_response = {
                "ì§ˆë¬¸": st.session_state.current_question,
                "ì‘ë‹µ": response,
            }
            st.session_state.chat_history[-1]["ì‘ë‹µ"] = response

            # íˆìŠ¤í† ë¦¬ ì €ì¥
            save_chat_history()

    except Exception as e:
        st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.session_state.chat_history[-1]["ì‘ë‹µ"] = "ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    finally:
        # ìƒíƒœ ê°±ì‹ 
        st.session_state.loading = False
        st.session_state.current_question = None

        # ì‘ë‹µ ìƒì„± í›„ íˆìŠ¤í† ë¦¬ ì»¨í…Œì´ë„ˆë¥¼ ìƒˆë¡œ ë Œë”ë§
        history_container.empty()  # ê¸°ì¡´ ë‚´ìš©ì„ ì§€ì›€
        display_history(history_container)  # ìƒˆë¡œ ë Œë”ë§

# í˜„ì¬ ì‘ë‹µ ì¶œë ¥
if st.session_state.loading:
    st.subheader("ì±—ë´‡ ì‘ë‹µ:")
    st.write("ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘ì…ë‹ˆë‹¤...")

elif st.session_state.current_response:
    st.subheader("ì±—ë´‡ ì‘ë‹µ:")
    st.write(f"**ì§ˆë¬¸:** {st.session_state.current_response['ì§ˆë¬¸']}")
    st.write(f"**ì‘ë‹µ:** {st.session_state.current_response['ì‘ë‹µ']}")
